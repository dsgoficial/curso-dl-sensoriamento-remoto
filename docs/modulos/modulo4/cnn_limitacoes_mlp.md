---
sidebar_position: 1
title: "O Paradigma de Visão Computacional antes das CNNs"
description: "As Limitações Fundamentais do Perceptron Multicamadas (MLP) para Processamento de Imagens"
tags: [mlp, limitações, visão computacional, dimensionalidade, flattening, correlação espacial]
---

# 1. O Paradigma de Visão Computacional antes das CNNs: As Limitações Fundamentais do Perceptron Multicamadas (MLP)

A ascensão das Redes Neurais Convolucionais (CNNs) no campo da visão computacional não foi um evento isolado, mas sim uma resposta direta às deficiências inerentes de arquiteturas de redes neurais mais antigas, como o Perceptron Multicamadas (MLP). Compreender essas limitações é fundamental para apreciar a inovação que as CNNs trouxeram para o processamento de imagens. O MLP, uma rede totalmente conectada, é uma das arquiteturas mais básicas do deep learning e é comumente utilizada para tarefas de regressão e classificação simples.¹ No entanto, sua estrutura revela-se profundamente inadequada para lidar com a natureza intrínseca dos dados de imagem.

## 1.1. Inadequação Estrutural para Dados Multidimensionais

A estrutura do MLP impõe uma exigência fundamental: todos os dados de entrada devem ser formatados como um vetor unidimensional. Para uma imagem, que é uma estrutura multidimensional (largura x altura x canais de cor), isso significa que a matriz de pixels deve ser "achatada" (o processo de flattening) em uma única linha de dados. Uma imagem de entrada, que originalmente possui a forma de uma matriz, é convertida em um vetor extenso. Por exemplo, uma imagem em escala de cinza de 28x28 pixels, como as do conjunto de dados MNIST, é transformada em um vetor de 784 elementos. Se a imagem for colorida, com três canais (RGB), a dimensão do vetor de entrada se torna ainda maior.

Essa conversão radical acarreta uma limitação crítica e insuperável para tarefas de visão computacional: a perda de informação espacial. Ao achatar a imagem, a correlação espacial entre pixels adjacentes é completamente desfeita. Um pixel que era vizinho de outro na imagem original pode acabar se tornando um vizinho distante no vetor de entrada. Essa informação, que descreve as relações entre pixels e forma padrões como bordas, contornos e texturas, é a essência da visão computacional. O MLP, por sua natureza, não se adapta a essa estrutura, pois trata cada pixel de forma independente, sem considerar seu contexto espacial ou a sua posição relativa na imagem. Como resultado, a rede é obrigada a aprender cada conexão de pixel de forma individual, ignorando a estrutura hierárquica e a correlação local que definem os objetos em uma imagem.

## 1.2. A "Maldição da Dimensionalidade" e a Necessidade Exponencial de Parâmetros

A inadequação estrutural do MLP se manifesta diretamente no fenômeno conhecido como a "maldição da dimensionalidade". A maldição da dimensionalidade refere-se a diversos desafios que surgem quando se trabalha com dados em espaços de alta dimensão, problemas que não ocorrem em ambientes de baixa dimensão, como o espaço físico tridimensional. No campo do aprendizado de máquina, este fenômeno se traduz na necessidade exponencial de dados de treinamento para que o modelo consiga generalizar de forma precisa. À medida que o número de dimensões aumenta, o volume do espaço aumenta tão rapidamente que os dados disponíveis se tornam extremamente esparsos, exigindo uma quantidade imensa de amostras para garantir que o modelo não memorizou apenas os dados existentes.

Esse problema impacta os MLPs de forma dramática no contexto de imagens. Cada pixel se torna uma dimensão de entrada para a rede. Para uma imagem de 28x28 pixels, o vetor de entrada tem 784 dimensões. Para imagens de alta resolução, o número de dimensões pode chegar a dezenas ou centenas de milhares. Como cada neurônio na primeira camada oculta de um MLP deve se conectar a cada uma dessas dimensões de entrada, o número de parâmetros (pesos e vieses) explode exponencialmente. Uma rede MLP simples, mesmo para uma imagem de resolução moderada, teria milhões de parâmetros, tornando o treinamento impraticável e a rede inviável para uso em escala. Essa necessidade de um "grande número de parâmetros" não é um mero problema de escala; é a manifestação direta da maldição da dimensionalidade. A falta de uma arquitetura que explore a correlação espacial local força a rede a aprender cada conexão individualmente, o que não apenas torna a rede computacionalmente custosa, mas também aumenta significativamente o risco de overfitting, onde o modelo memoriza os dados de treinamento em vez de aprender padrões que podem ser aplicados a novos dados.